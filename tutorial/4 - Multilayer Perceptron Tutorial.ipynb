{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron activity\n",
    "\n",
    "In this activity, you will make a multi-layer perceptron (MLP) model in the PyTorch deep learning package to perform classification of hand-written digits in the classic MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Main torch import for torch tensors\n",
    "import torch.nn as nn  # Neural network module for building deep learning models\n",
    "import torch.nn.functional as F  # Functional module, includes activation functions\n",
    "import torch.optim as optim  # Optimization module\n",
    "import torchvision  # Vision / image processing package built on top of torch\n",
    "\n",
    "from matplotlib import pyplot as plt  # Plotting and visualization\n",
    "from sklearn.metrics import accuracy_score  # Computing accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common practice to normalize input data to neural networks (0 mean, unit variance)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # All inputs to PyTorch neural networks must be torch.Tensor\n",
    "    torchvision.transforms.Normalize(mean=0.1307, std=0.3081)  # Subtracts mean and divides by std. Note that the raw data is between [0, 1]\n",
    "])\n",
    "\n",
    "# Download the MNIST data and lazily apply the transformation pipeline\n",
    "mnist = torchvision.datasets.MNIST(root='.', download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Setup data loaders\n",
    "# Note: Iterating through the dataloader yields batches of (inputs, targets)\n",
    "# where inputs is a torch.Tensor of shape (B, 1, 28, 28) and targets is a torch.Tensor of shape (B,)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(mnist, torch.arange(5000)), \n",
    "    batch_size=1000, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(mnist, torch.arange(5000, 10000)), \n",
    "    batch_size=1000,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5, figsize=(5, 6))\n",
    "\n",
    "plot_images = []\n",
    "plot_labels = []\n",
    "\n",
    "for i, ax in enumerate(axs.flatten(), start=1000):\n",
    "    (image, label) = mnist[i]\n",
    "\n",
    "    # Save this data for later\n",
    "    plot_images.append(image)\n",
    "    plot_labels.append(label)\n",
    "\n",
    "    # Plot each image\n",
    "    ax.imshow(image.squeeze(), cmap=\"viridis\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plot_images = torch.cat(plot_images)  # Combine all the images into a single batch for later\n",
    "\n",
    "print(f\"Each image is a torch.Tensor and has shape {image.shape}.\")\n",
    "print(f\"The labels are the integers 0 to 9, representing the digits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the MLP model\n",
    "\n",
    "Although we draw diagrams of hidden layers as neurons with incoming and outgoing connections, in practice, we implement this with two linear layers (also called \"dense layers\") and a pointwise non-linearity in between. The first layer is a linear transform (matrix multiplication) from the input dimension to the hidden dimension. The second layer is a linear transform from the hidden dimension to the output dimension.\n",
    "\n",
    "For this model, the input dimension is (28*28) as we will flatten the 2D images into a 1D vector. The hidden dimension is 100. The output dimension is 10, since we have 10 classes. We will use the ReLU non-linearity.\n",
    "\n",
    "In PyTorch, a model is defined by subclassing the `nn.Module` class and we define behaviour in two methods. In the `__init__` method, we setup the model architecture such as number layers, the size of each layer, etc. In the `forward()` method, we define the operations performed by the model's layers on the input data to produce outputs.\n",
    "\n",
    "Note: You do not need to apply a softmax to the outputs as this is automatically done with the appropriate loss function.\n",
    "\n",
    "Relevant documentation:\n",
    "\n",
    "- [PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "\n",
    "- [PyTorch activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "\n",
    "- [PyTorch linear layer documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Assign self.hidden to a torch linear layer of the correct size\n",
    "        self.hidden = None\n",
    "        # TODO: Assign self.output to a torch linear layer of the correct size\n",
    "        self.output = None\n",
    "        #####\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass implementation for the network\n",
    "        \n",
    "        :param x: torch.Tensor of shape (batch, 1, 28, 28), input images\n",
    "\n",
    "        :returns: torch.Tensor of shape (batch, 10), output logits\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1)  # shape (batch, 28*28)\n",
    "        # TODO: Process x through self.hidden, relu, and self.output and return the result\n",
    "        return x\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs are of shape (2, 1, 28, 28) and we expect an output tensor of shape (2, 10)\n",
    "\n",
    "mlp = MultiLayerPerceptron()\n",
    "x = torch.randn(2, 1, 28, 28)\n",
    "z = mlp(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup optimizer and loss function\n",
    "\n",
    "In Assignment 2, we use the stochastic gradient descent (SGD) optimizer with learning rate 0.1 and momentum 0.5. We will train this model for 5 epochs.\n",
    "\n",
    "The task we are performing is multiclass classification (10 independent classes, one for each digit). The loss function to use for this task is cross entropy loss.\n",
    "\n",
    "Relevant documentation:\n",
    "- [PyTorch optimizers](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "- [PyTorch loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate your model and setup the optimizer\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.5\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup the appropriate loss function for this task\n",
    "loss_fn = None\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup training loop\n",
    "\n",
    "During the training loop, we perform the following steps:\n",
    "\n",
    "1. Fetch the next batch of inputs and targets from the dataloader\n",
    "2. Zero the parameter gradients\n",
    "3. Compute the model output predictions from the inputs\n",
    "4. Compute the loss between the model outputs and the targets\n",
    "5. Compute the parameter gradients with backpropagation\n",
    "6. Perform a gradient descent step with the optimizer to update the model parameters\n",
    "\n",
    "Relevant documentation:\n",
    "- [PyTorch optimization step](https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epoch=-1):\n",
    "    \"\"\"\n",
    "    Trains a model for one epoch (one pass through the entire training data).\n",
    "\n",
    "    :param model: PyTorch model\n",
    "    :param train_loader: PyTorch Dataloader for training data\n",
    "    :param loss_fn: PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer, initialized with model parameters\n",
    "    :kwarg epoch: Integer epoch to use when printing loss and accuracy\n",
    "    :returns: Accuracy score\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.train()  # Set model in training mode\n",
    "    for i, (inputs, targets) in enumerate(train_loader):  # 1. Fetch next batch of data\n",
    "        # TODO: Fill in the rest of the training loop\n",
    "        # 2. Zero parameter gradients\n",
    "        # 3. Compute model outputs\n",
    "        # 4. Compute loss between outputs and targets\n",
    "        # 5. Backpropagation for parameter gradients\n",
    "        # 6. Gradient descent step\n",
    "        #####\n",
    "\n",
    "        # Track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)  # Take the class with the highest output as the prediction\n",
    "\n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_targets.extend(targets.tolist())\n",
    "\n",
    "        # Print some statistics every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            running_loss = total_loss / (i + 1)\n",
    "            print(f\"Epoch {epoch + 1}, batch {i + 1}: loss = {running_loss:.2f}\")\n",
    "\n",
    "    # TODO: Compute the overall accuracy        \n",
    "    acc = None\n",
    "    #####\n",
    "\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. Average train loss = {total_loss / len(train_loader):.2f}, average train accuracy = {acc * 100:.3f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In testing, we don't need to compute gradients or do an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_fn, epoch=-1):\n",
    "    \"\"\"\n",
    "    Tests a model for one epoch of test data.\n",
    "\n",
    "    Note:\n",
    "        In testing and evaluation, we do not perform gradient descent optimization, so steps 2, 5, and 6 are not needed.\n",
    "        For performance, we also tell torch not to track gradients by using the `with torch.no_grad()` context.\n",
    "\n",
    "    :param model: PyTorch model\n",
    "    :param test_loader: PyTorch Dataloader for test data\n",
    "    :param loss_fn: PyTorch loss function\n",
    "    :kwarg epoch: Integer epoch to use when printing loss and accuracy\n",
    "\n",
    "    :returns: Accuracy score\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.eval()  # Set model in evaluation mode\n",
    "    for i, (inputs, targets) in enumerate(test_loader):  # 1. Fetch next batch of data\n",
    "        with torch.no_grad():\n",
    "            # TODO: Compute the model outputs and loss only. Do not update using the optimizer\n",
    "            # 3. Compute model outputs\n",
    "            # 4. Compute loss between outputs and targets\n",
    "            #####\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)  # Take the class with the highest output as the prediction\n",
    "            all_predictions.extend(preds.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. Average test loss = {total_loss / len(test_loader):.2f}, average test accuracy = {acc * 100:.3f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = []\n",
    "test_metrics = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # TODO: Fill in the rest of the arguments to the train and test functions\n",
    "    train_acc = train(...)\n",
    "    test_acc = test(...)\n",
    "\n",
    "    train_metrics.append(train_acc)\n",
    "    test_metrics.append(test_acc)\n",
    "    #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visually compare the model predictions\n",
    "\n",
    "We will lastly see the trained model's predictions on the 20 examples we visualized in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the plot_images\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plot_outputs = model(plot_images)\n",
    "    plot_preds = torch.argmax(plot_outputs, dim=-1)\n",
    "\n",
    "# Plot and show the labels\n",
    "fig, axs = plt.subplots(4, 5, figsize=(7, 8))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    image = plot_images[i]\n",
    "    label = plot_labels[i]\n",
    "    pred = plot_preds[i]\n",
    "\n",
    "    ax.imshow(image.squeeze(), cmap=\"viridis\")\n",
    "    ax.set_title(f\"Prediction: {pred}\\nLabel: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "xs = 1 + torch.arange(NUM_EPOCHS)\n",
    "plt.plot(xs, train_metrics, \"o-\", label=\"Train accuracy\")\n",
    "plt.plot(xs, test_metrics, \"o-\", label=\"Test accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
