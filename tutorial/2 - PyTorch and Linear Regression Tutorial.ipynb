{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression activity\n",
    "\n",
    "The data on LEARN are measurements of the encoded response a neuron generates when a visual stimulus presented at different angles. The independent variable is the stimulus angle in radians and the dependent variable is sensory specific information in bits. The data was digitized from figure 1.B in [1].\n",
    "\n",
    "In this activity, you will fit a regression model to this data using PyTorch. As you will see in the data plot, the curve is non-linear, so a Gaussian basis function (kernel) will be used to generate the features for the regression.\n",
    "\n",
    "[1] D. A. Butts and M. S. Goldman, “Tuning curves, neuronal variability, and sensory coding,” PLoS Biology, vol. 4, no. 4, 2006. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/trevor-yu-087/syde-552-w24-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the filepath to point the regression activity data\n",
    "filepath = \"syde-552-w24-tutorial/tutorial/2 - regression activity data.pkl\"\n",
    "#####\n",
    "\n",
    "with open(filepath, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "x = data[\"angle\"]\n",
    "y = data[\"ssi\"]\n",
    "\n",
    "print(f\"x is an array of shape {x.shape}\")\n",
    "print(f\"y is an array of shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel(\"Stimulus angle [radians]\")\n",
    "plt.ylabel(\"SSI [bits]\")\n",
    "plt.title(\"Neuron information encoding curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into data torch float tensors\n",
    "inputs = torch.FloatTensor(x)\n",
    "targets = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Gaussian kernels and broadcasting\n",
    "\n",
    "The formula for the Gaussian function, given parameters $\\mu$ and $\\sigma$, is:\n",
    "\n",
    "$$\\phi(x | \\mu, \\sigma) = e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}$$\n",
    "\n",
    "We will take a look at how to do this with broadcasting, so that we don't need to use loops to generate these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is incompatible with broadcasting, last dim doesn't fit rules\n",
    "a = torch.ones(5)  # Shape (5,)\n",
    "mu = torch.arange(3)  # Shape (3,)\n",
    "a + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is compatible with broadcasting, last dims match\n",
    "a = torch.ones((5, 3))  # Shape (5, 3)\n",
    "mu = torch.arange(3)  # Shape (3,)\n",
    "a + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also compatible with broadcasting, last dims of a is 1\n",
    "a = torch.ones((5, 1))  # Shape (5, 1)\n",
    "mu = torch.arange(3)  # Shape (3,)\n",
    "a + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.linspace(0, 5, 5)  # Shape (5,)\n",
    "sigma = torch.ones(5)  # Shape (5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Gaussian kernels and broadcasting\n",
    "\n",
    "The formula for the Gaussian function, given parameters $\\mu$ and $\\sigma$, is:\n",
    "\n",
    "$$\\phi(x | \\mu, \\sigma) = e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}$$\n",
    "\n",
    "We will take a look at how to do this with broadcasting, so that we don't need to use loops to generate these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_features(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x: shape (B, 1)\n",
    "        mu: shape (D,)\n",
    "        sigma: shape (D,)\n",
    "\n",
    "    Returns:\n",
    "        phi: shape (B, D)    \n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(-1)  # Shape (B, 1)\n",
    "    ## TODO: Write the equation to compute the Gaussian features\n",
    "    phi = None\n",
    "    ###\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we generate 5 kernels with standard deviation of 0.5\n",
    "D = 5\n",
    "x = torch.linspace(-2, 2, 100)\n",
    "mu = torch.linspace(-1.5, 1.5, D)\n",
    "\n",
    "std = 0.5\n",
    "sigma = torch.ones(D) * std\n",
    "phi = get_gaussian_features(x, mu, sigma)\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(D):\n",
    "    plt.plot(x, phi[:, i])\n",
    "\n",
    "plt.plot(inputs, targets, label=\"Target curve\", c=\"k\")\n",
    "plt.title(f\"{D} Gaussians with std={std}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find a number of bases and their width that would be suitable for this problem. We ideally want to tile the input space with functions whose widths are sized appropriately to model interesting features of the target curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Experiment with different values of D and std to find a set of kernels that spans the space well.\n",
    "D = 15\n",
    "std = 0.2\n",
    "###\n",
    "\n",
    "x = torch.linspace(-2, 2, 100)\n",
    "mu = torch.linspace(-1.5, 1.5, D)\n",
    "\n",
    "sigma = torch.ones(D) * std\n",
    "phi = get_gaussian_features(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(D):\n",
    "    plt.plot(x, phi[:, i])\n",
    "\n",
    "plt.plot(inputs, targets, label=\"Target curve\", c=\"k\")\n",
    "plt.title(f\"{D} Gaussians with std={std}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression and matrix multiplication\n",
    "For linear regression, we have the following equation, where $\\hat{y} \\in \\mathbb{R}, W \\in \\mathbb{R}^{D \\times 1}, \\vec{\\phi}(x) = \\mathbb{R}^{D}$.\n",
    "\n",
    "$$\\hat{y} = \\sum_i^D \\phi_i(x)w_i = \\vec{\\phi}(x) W$$\n",
    "\n",
    "If we have a batch of $N$ targets and vectors, the equation mostly looks the same, but the shapes are different. $\\hat{Y} \\in \\mathbb{R}^{N \\times 1}, W \\in \\mathbb{R}^{D \\times 1}, \\Phi \\in \\mathbb{R}^{N \\times D}$.\n",
    "\n",
    "$$\\hat{Y} = \\Phi W$$\n",
    "\n",
    "Note, this equation is matrix muliplication, where $\\Phi$ has shape $(B, D)$ and $W$ has shape $(D, 1)$, resulting in $\\hat{Y}$ of shape $(B, 1)$. The inner dimension of the two matrices match, which fits the rules for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ues mu and sigma as you found above\n",
    "features = get_gaussian_features(inputs, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "Y = targets.unsqueeze(-1)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we set D above when we generated the Gaussians\n",
    "W = torch.randn((D, 1))\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Compute the model outputs, Y_hat, with matrix multiplication\n",
    "Y_hat = None\n",
    "###\n",
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error and reduction\n",
    "\n",
    "The error for a single example is typically represented as:\n",
    "\n",
    "$$E(W) = \\frac{1}{2} (\\hat{y} - y)^2 = \\frac{1}{2} (\\vec{\\phi}(x) W - y)^2$$\n",
    "\n",
    "However, we typically compute the error over a batch of examples. This equation gives a sense of how to do this operation in PyTorch, recalling the batch dimension was the first dimension of these tensors.\n",
    "\n",
    "$$E(W) = \\sum_i^B \\frac{1}{2} (\\vec{\\phi}(x_i) W - y_i)^2 = \\frac{1}{2} \\sum_{\\text{dim=0}} (\\Phi W - Y)^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compuate the error of this random weight matrix\n",
    "Y_hat = X @ W\n",
    "## TODO: Compute the error\n",
    "error = None\n",
    "###\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression with a random weight matrix\n",
    "plt.plot(inputs, Y, label=\"Target curve\", c=\"k\")\n",
    "plt.plot(inputs, Y_hat, label=\"Estimated curve, random weights\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try yourself: Use equation for MLE to find optimal weights\n",
    "The MLE estimator formula for optimal weight matrix, $W$ is:\n",
    "\n",
    "$$W^{*} = (X^{\\top}X)^{-1} X^{\\top}Y$$\n",
    "\n",
    "Use can use `torch.inverse(X)` to compute the matrix inverse and `X.T` to get the transpose.\n",
    "\n",
    "Compute the optimal error and plot the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Compute optimal weights with MLE formula\n",
    "W_star = torch.inverse(X.T @ X) @ (X.T @ Y)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Compute the error with optimal weights\n",
    "Y_hat_opt = None\n",
    "error_opt = None\n",
    "###\n",
    "error_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs, Y, label=\"Target curve\", c=\"k\")\n",
    "plt.plot(inputs, Y_hat_opt, label=\"Estimated curve (optimal weights)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
